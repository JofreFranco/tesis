{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38764bitpyenv389060af4eb43a4d22820780620fff68d6",
   "display_name": "Python 3.8.7 64-bit ('.PyEnv38')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Prepocesamiento y preparación del dataset.\n",
    "\n",
    "En primer lugar los audios se convierten a formato mp3 para que estén igual a las grabaciones del call center. Después se aplica un filtro de 300 a 3300 Hz porque es el rango de frecuencias en el que trabajan los codecs de audio más utilizados para telefonía IP. Por último, a los audios se les calcula el los features.\n",
    "Los casos positivos se incrementan agregando ruido y desplazándolos en tiempo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "from pydub import AudioSegment,silence\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "import librosa\n",
    "import statistics\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import lfilter, butter\n",
    "import os"
   ]
  },
  {
   "source": [
    "\n",
    "- ### Features:\n",
    "    - mfcc  longitud: 40 [0:40]\n",
    "\n",
    "    - mfcc_delta1  longitud: 40 [40:80]\n",
    "\n",
    "    - mfcc_delta2  longitud: 40 [80:120]\n",
    "\n",
    "    - meanF0  longitud: 1 [120]\n",
    "\n",
    "    - stdevF0  longitud: 1 [121]\n",
    "\n",
    "    - hnr  longitud: 1 [122]\n",
    "\n",
    "    - f_means  longitud: 4 (f1, f2, f3, f4) Formantes [123:127] No esta habilitada porque no andaba para todos los archivos\n",
    "\n",
    "    - f_medians  longitud: 4 (f1, f2, f3, f4) Formantes   [127:131] No esta habilitada porque no andaba para todos los archivos\n",
    "\n",
    "    - spectral_centroid  longitud: 121 [122:243]\n",
    "\n",
    "    - spectral_rollof  longitud: 121 [243:364]\n",
    "\n",
    "    - zero_crossing_rate  longitud: 121 [364:485]\n",
    "\n",
    "    - Longitud total del vector de features: 494\n",
    "        "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Funciones de procesamiento y data augmentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro para igual el ancho de banda del codec de comunicación IP\n",
    "\n",
    "def butter_params(fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low_freq = 300\n",
    "    high_freq = 3300\n",
    "    low = low_freq / nyq\n",
    "    high = high_freq / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(audio, fs, order=5):\n",
    "    b, a = butter_params(fs, order=order)\n",
    "    y = lfilter(b, a, audio)\n",
    "    return y\n",
    "\n",
    "# Aumentado del dataset (Agregado de ruido y desplazamiento temporal)\n",
    "\n",
    "def add_noise(data):\n",
    "    max_amp = 0.15\n",
    "    rand_amp = random.randrange(80,100) / (100 / max_amp)\n",
    "    noise_amp = rand_amp*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def shift_time(audio, sr, max_shift):\n",
    "    shift = 0\n",
    "    while shift == 0:\n",
    "        max_shift = max_shift * sr\n",
    "        direction = random.randrange(-1, 2, 2)  # -1 = right, 1 = left\n",
    "        shift = np.random.randint(max_shift) * direction\n",
    "        if shift == 0:\n",
    "            continue\n",
    "        audio = np.roll(audio, shift)\n",
    "        if direction == -1:\n",
    "            audio[shift:] = 0\n",
    "        else:\n",
    "            audio[:shift] = 0\n",
    "\n",
    "        return audio\n",
    "\n",
    "def augment_data(audio, sr):\n",
    "    noise_audio = add_noise(audio)\n",
    "    shifted_audio = shift_time(audio, sr, len(audio)*0.5/sr)\n",
    "    return noise_audio, shifted_audio\n",
    "\n",
    "# Funcion para cargar dataset una vez guardado\n",
    "\n",
    "def load_dataset(dataset_name, mix=True):\n",
    "    dataset = np.load(dataset_name, allow_pickle=True)\n",
    "    X = dataset[()]['x']\n",
    "    Y = dataset[()]['y']\n",
    "    if mix:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(np.array(X), np.array(Y), test_size=0.1, random_state=9)\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    else:\n",
    "        return X, Y"
   ]
  },
  {
   "source": [
    "#### Funciones de extracción de features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crest_factor_RMS(sound):\n",
    "    rms = np.mean(librosa.feature.rms(sound))\n",
    "    peak = max(np.abs(sound))\n",
    "    crest_factor = peak / rms\n",
    "    return crest_factor, rms\n",
    "\n",
    "\n",
    "def measureFormants(sound, f0min, f0max):\n",
    "    pointProcess = call(sound, \"To PointProcess (periodic, cc)\", f0min, f0max)\n",
    "\n",
    "    formants = call(sound, \"To Formant (burg)\", 0.0025, 5, 5000, 0.025, 50)\n",
    "    numPoints = call(pointProcess, \"Get number of points\")\n",
    "\n",
    "    f1_list = []\n",
    "    f2_list = []\n",
    "    f3_list = []\n",
    "    f4_list = []\n",
    "\n",
    "    # Measure formants only at glottal pulses\n",
    "    for point in range(0, numPoints):\n",
    "        point += 1\n",
    "        t = call(pointProcess, \"Get time from index\", point)\n",
    "\n",
    "        f1 = call(formants, \"Get value at time\", 1, t, \"Hertz\", \"Linear\")\n",
    "        f2 = call(formants, \"Get value at time\", 2, t, \"Hertz\", \"Linear\")\n",
    "        f3 = call(formants, \"Get value at time\", 3, t, \"Hertz\", \"Linear\")\n",
    "        f4 = call(formants, \"Get value at time\", 4, t, \"Hertz\", \"Linear\")\n",
    "\n",
    "        f1_list.append(f1)\n",
    "        f2_list.append(f2)\n",
    "        f3_list.append(f3)\n",
    "        f4_list.append(f4)\n",
    "\n",
    "    f1_list = [f1 for f1 in f1_list if str(f1) != \"nan\"]\n",
    "    f2_list = [f2 for f2 in f2_list if str(f2) != \"nan\"]\n",
    "    f3_list = [f3 for f3 in f3_list if str(f3) != \"nan\"]\n",
    "    f4_list = [f4 for f4 in f4_list if str(f4) != \"nan\"]\n",
    "\n",
    "    # calculate mean formants across pulses\n",
    "    f1_mean = statistics.mean(f1_list)\n",
    "    f2_mean = statistics.mean(f2_list)\n",
    "    f3_mean = statistics.mean(f3_list)\n",
    "    f4_mean = statistics.mean(f4_list)\n",
    "    f_means = [f1_mean, f2_mean, f3_mean, f4_mean]\n",
    "    # calculate median formants across pulses, this is what is used in all subsequent calcualtions\n",
    "    # you can use mean if you want, just edit the code in the boxes below to replace median with mean\n",
    "    f1_median = statistics.median(f1_list)\n",
    "    f2_median = statistics.median(f2_list)\n",
    "    f3_median = statistics.median(f3_list)\n",
    "    f4_median = statistics.median(f4_list)\n",
    "    f_medians = [f1_median, f2_median, f3_median, f4_median]\n",
    "    return f_means, f_medians\n",
    "\n",
    "\n",
    "def get_mfccs(y, sr, n_mfcc):\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_delta1 = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc_delta1)\n",
    "    mfcc = np.mean(mfcc, axis=1)\n",
    "    mfcc_delta1 = np.mean(mfcc_delta1, axis=1)\n",
    "    mfcc_delta2 = np.mean(mfcc_delta2, axis=1)\n",
    "    return mfcc, mfcc_delta1, mfcc_delta2\n",
    "\n",
    "\n",
    "def feature_extraction(audio, sr, f0min, f0max, n_mfcc, unit=\"Hertz\" ):\n",
    "    \n",
    "    f0, _, _ = librosa.pyin(\n",
    "        audio, fmin=librosa.note_to_hz(\"C2\"), fmax=librosa.note_to_hz(\"C7\")\n",
    "    )\n",
    "    if len(f0)<9:\n",
    "        return 'skip'\n",
    "    f0_delta = librosa.feature.delta(f0)\n",
    "    meanF0 = np.nanmean(f0)\n",
    "    stdevF0 = np.nanstd(f0)\n",
    "    meanF0delta = np.nanmean(f0_delta)\n",
    "\n",
    "    if np.isnan(meanF0):\n",
    "        return 'skip'\n",
    "\n",
    "    sound = parselmouth.Sound(audio, sr)  # read the sound\n",
    "    pitch = call(sound, \"To Pitch\", 0.0, f0min, f0max)  # create a praat pitch object\n",
    "\n",
    "    \n",
    "\n",
    "    harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, f0min, 0.1, 1.0)\n",
    "    hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "\n",
    "    # f_means, f_medians = measureFormants(sound, f0min, f0max)  # Formantes\n",
    "\n",
    "    mfcc, mfcc_delta1, mfcc_delta2 = get_mfccs(audio, sr, n_mfcc)\n",
    "    crest_factor, rms = get_crest_factor_RMS(audio)\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(audio, sr=sr))\n",
    "    spectral_rollof = np.mean(\n",
    "        librosa.feature.spectral_rolloff(audio, sr=sr, roll_percent=0.85)\n",
    "    )\n",
    "\n",
    "    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(audio, sr))\n",
    "    output = np.concatenate([mfcc,\n",
    "        mfcc_delta1,\n",
    "        mfcc_delta2,\n",
    "        np.array([meanF0]),\n",
    "        np.array([stdevF0]),\n",
    "        np.array([meanF0delta]),\n",
    "        np.array([hnr]),\n",
    "        np.array([crest_factor]),\n",
    "        np.array([rms]),\n",
    "        # np.array(f_means),\n",
    "        # np.array(f_medians),\n",
    "        np.array([spectral_centroid]),\n",
    "        np.array([spectral_rollof]),\n",
    "        np.array([zero_crossing_rate])])\n",
    "    output = list(output)\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "#### Armado del dataset completo\n",
    "\n",
    "- Se eliminan los audios de menos de 1 segundo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_normalization(dataset, n_mfcc):\n",
    "    dataset = np.array(dataset)\n",
    "    for n in range(3):\n",
    "        mean = np.mean(dataset[:, n * n_mfcc : (n + 1) * n_mfcc])\n",
    "        std = np.std(dataset[:, n * n_mfcc : (n + 1) * n_mfcc])\n",
    "        dataset[:, n * n_mfcc : (n + 1) * n_mfcc] = (\n",
    "            dataset[:, n * n_mfcc : (n + 1) * n_mfcc] - mean\n",
    "        ) / std\n",
    "    N = n_mfcc * 3\n",
    "    mean = np.mean(dataset[:, N:], axis=0)\n",
    "    std = np.std(dataset[:, N:], axis=0)\n",
    "    dataset[:, N:] = (dataset[:, N:] - mean) / std\n",
    "    return dataset\n",
    "\n",
    "def process_dataset(directory, name,n_mfcc,n_start, augment=True ):\n",
    "    f0min = 300\n",
    "    f0max = 3300\n",
    "    directory = directory + '*/*.mp3'\n",
    "    files = glob.glob(directory)\n",
    "    anger = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    ext = '.npy'\n",
    "    files = files[n_start :]\n",
    "    try:\n",
    "        for n, path in enumerate(files):\n",
    "            if n % 1000 == 0:\n",
    "                print(n)\n",
    "            if n % 100 == 0 and n != 0:\n",
    "                dataset = {'x': X, 'y': Y }\n",
    "                np.save('datasets/' + name + f'_{n}' + ext, dataset)\n",
    "                X = []\n",
    "                Y = []\n",
    "\n",
    "            audio, sr = librosa.load_mp3(path)\n",
    "\n",
    "            audio = butter_bandpass_filter(audio, sr, order=5)\n",
    "            features = feature_extraction(audio, sr, f0min, f0max,n_mfcc, unit=\"Hertz\" )\n",
    "            if features == 'skip':\n",
    "                continue \n",
    "            \n",
    "            file_name = os.path.basename(path)\n",
    "            file_name = file_name.replace(\"-\", \"_\")\n",
    "        \n",
    "            emotion = file_name.split('_')[2]\n",
    "            if len(list(audio))/sr < 0.5:\n",
    "                continue\n",
    "            if emotion == '05' or emotion == 'anger.mp3':\n",
    "                emotion = 1\n",
    "                anger = anger + 1\n",
    "            else:\n",
    "                emotion = 0\n",
    "            X.append(features)\n",
    "            Y.append(emotion)\n",
    "            \n",
    "            if emotion == 1 and augment:\n",
    "                augmented_data = augment_data(audio, sr)\n",
    "                for audio in augmented_data:\n",
    "                    audio = butter_bandpass_filter(audio, sr, order=5)\n",
    "                    features = feature_extraction(audio, sr, f0min, f0max,n_mfcc, unit=\"Hertz\" )\n",
    "                    anger = anger + 1\n",
    "                    X.append(features)\n",
    "                    Y.append(emotion)\n",
    "    except Exception as e:\n",
    "        print(path)\n",
    "        raise(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-979c64c326a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/francoj/Documentos/Reconocimiento de emociones/tesis/Data/*/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "directory = glob.glob(\"/home/francoj/Documentos/Reconocimiento de emociones/tesis/Data/*/\")\n",
    "n = 1\n",
    "for dataset in directory:\n",
    "    if n == 1:\n",
    "        continue\n",
    "        n = 0\n",
    "    name = dataset.split(\"/\")[-2]\n",
    "    process_dataset(dataset, name, 16, 400)\n",
    "    print('terminado ', name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'x': x, 'y': y }\n",
    "np.save('prueba.npy', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}